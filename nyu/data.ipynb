{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)-20s %(message)s', datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "logger = logging.getLogger(\"data_formatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "DATA_V0  = \"../data/v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrainData():\n",
    "    logger.debug(\"Reading training data\")\n",
    "    inp_file = pd.read_csv(f\"{DATA_DIR}/train_input.csv\")\n",
    "    X = inp_file.iloc[:,0:4].values\n",
    "    \n",
    "    with np.load(f\"{DATA_DIR}/train_output.npz\") as out_file:\n",
    "        Y = [out_file[key] for key in out_file]\n",
    "    \n",
    "    return len(X), X, Y\n",
    "\n",
    "def readTestData():\n",
    "    logger.debug(\"Reading test data\")\n",
    "    inp_file = pd.read_csv(f\"{DATA_DIR}/test_input.csv\")\n",
    "    X = inp_file.iloc[:,0:4].values\n",
    "    return len(X), X\n",
    "\n",
    "def readTrainDataV0(tag):\n",
    "    with open(f\"{DATA_V0}/train_data_{tag}.pkl\", \"rb\") as file:\n",
    "        n, X, Y = pk.load(file)\n",
    "    return n, X, Y\n",
    "\n",
    "def readTestDataV0():\n",
    "    with open(f\"{DATA_V0}/test_data.pkl\", \"rb\") as file:\n",
    "        n, X = pk.load(file)\n",
    "    return n, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data_close_v0(T = 30):\n",
    "    \"\"\" Reads data from DATA_DIR and writes to DATA_V0 \"\"\"\n",
    "    n, X, Y = readTrainData()\n",
    "    \n",
    "    # Reduce \"ACDEFGHIKLMNPQRSTVWXY\" and \"BEGHILST\"\n",
    "    D1 = {ch: chr(ord('A')+i) for i, ch in enumerate(\"ACDEFGHIKLMNPQRSTVWXY\")}\n",
    "    D2 = {ch: chr(ord('A')+i) for i, ch in enumerate(\"BEGHILST\")}\n",
    "\n",
    "    logger.debug(\"Formatting training data\")\n",
    "    for k in range(n):\n",
    "        length = X[k][1]\n",
    "        str1 = ''\n",
    "        str2 = ''\n",
    "        for i in range(length):\n",
    "            str1 += D1[X[k][2][i]]\n",
    "            str2 += D2[X[k][3][i]]\n",
    "        X[k][2], X[k][3] = str1, str2\n",
    "    \n",
    "    for k in tqdm.tqdm(range(n)):\n",
    "        length = X[k][1]\n",
    "        for i in range(length):\n",
    "            for j in range(i,length):\n",
    "                if j - i >= 30 and Y[k][i,j] <= T:\n",
    "                    Y[k][i,j] = 1.0\n",
    "                else:\n",
    "                    Y[k][i,j] = 0.0\n",
    "    \n",
    "    n_train = int(n * 0.8)\n",
    "    n_test  = n - n_train\n",
    "    X_train, X_test = X[:n_train], X[n_train:]\n",
    "    Y_train, Y_test = Y[:n_train], Y[n_train:]\n",
    "    \n",
    "    logger.debug(\"Pickling formatted training data\")\n",
    "    path = f\"{DATA_V0}\"\n",
    "    if not os.path.exists(path): os.makedirs(path)\n",
    "    \n",
    "    with open(f\"{DATA_V0}/train_data_close_100.pkl\", \"wb\") as file:\n",
    "        pk.dump((n,X,Y),file, protocol=0)\n",
    "    \n",
    "    with open(f\"{DATA_V0}/train_data_close_80.pkl\", \"wb\") as file:\n",
    "        pk.dump((n_train,X_train,Y_train),file, protocol=0)\n",
    "    \n",
    "    with open(f\"{DATA_V0}/train_data_close_20.pkl\", \"wb\") as file:\n",
    "        pk.dump((n_test,X_test,Y_test),file, protocol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data_v0():\n",
    "    \"\"\" Reads data from DATA_DIR and writes to DATA_V0 \"\"\"\n",
    "    n, X, Y = readTrainData()\n",
    "    \n",
    "    # Reduce \"ACDEFGHIKLMNPQRSTVWXY\" and \"BEGHILST\"\n",
    "    D1 = {ch: chr(ord('A')+i) for i, ch in enumerate(\"ACDEFGHIKLMNPQRSTVWXY\")}\n",
    "    D2 = {ch: chr(ord('A')+i) for i, ch in enumerate(\"BEGHILST\")}\n",
    "\n",
    "    logger.debug(\"Formatting training data\")\n",
    "    for k in range(n):\n",
    "        length = X[k][1]\n",
    "        str1 = ''\n",
    "        str2 = ''\n",
    "        for i in range(length):\n",
    "            str1 += D1[X[k][2][i]]\n",
    "            str2 += D2[X[k][3][i]]\n",
    "        X[k][2], X[k][3] = str1, str2\n",
    "    \n",
    "    n_train = int(n * 0.8)\n",
    "    n_test  = n - n_train\n",
    "    X_train, X_test = X[:n_train], X[n_train:]\n",
    "    Y_train, Y_test = Y[:n_train], Y[n_train:]\n",
    "\n",
    "    logger.debug(\"Pickling formatted training data\")\n",
    "    path = f\"{DATA_V0}\"\n",
    "    if not os.path.exists(path): os.makedirs(path)\n",
    "    \n",
    "    with open(f\"{DATA_V0}/train_data_100.pkl\", \"wb\") as file:\n",
    "        pk.dump((n,X,Y),file, protocol=pk.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open(f\"{DATA_V0}/train_data_80.pkl\", \"wb\") as file:\n",
    "        pk.dump((n_train,X_train,Y_train),file, protocol=pk.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open(f\"{DATA_V0}/train_data_20.pkl\", \"wb\") as file:\n",
    "        pk.dump((n_test,X_test,Y_test),file, protocol=pk.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    del n, X, Y\n",
    "    del n_train, X_train, Y_train\n",
    "    del n_test, X_test, Y_test\n",
    "    \n",
    "    n, X = readTestData()\n",
    "    for k in range(n):\n",
    "        length = X[k][1]\n",
    "        str1 = ''\n",
    "        str2 = ''\n",
    "        for i in range(length):\n",
    "            str1 += D1[X[k][2][i]]\n",
    "            str2 += D2[X[k][3][i]]\n",
    "        X[k][2], X[k][3] = str1, str2\n",
    "    \n",
    "    with open(f\"{DATA_V0}/test_data.pkl\", \"wb\") as file:\n",
    "        pk.dump((n,X),file, protocol=pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataV0_length():\n",
    "    file = pd.read_csv('Data/data_distance.csv')\n",
    "    X = file.values\n",
    "    n = X.shape[0]\n",
    "    y_avg = np.zeros(691)\n",
    "    for k in range(n):\n",
    "        y_avg[k+1] = X[k,3]\n",
    "    file = open('Data2/V0/y_avg.pickle','wb')\n",
    "    pk.dump(y_avg,file, protocol=0)\n",
    "    file.close()\n",
    "\n",
    "def DataLength():\n",
    "    n, X, _ = readTrainData()\n",
    "    MinL = 10000\n",
    "    MaxL = 0\n",
    "    for k in range(n):\n",
    "        MinL = min(MinL, X[k][1])\n",
    "        MaxL = max(MaxL, X[k][1])\n",
    "    \n",
    "    n, X = readTestData()\n",
    "    for k in range(n):\n",
    "        MinL = min(MinL, X[k][1])\n",
    "        MaxL = max(MaxL, X[k][1])\n",
    "    # MinL = 12, MaxL = 691\n",
    "    print(MinL,MaxL)\n",
    "\n",
    "def DataDistance():\n",
    "    n, _, Y = readTrainDataV0('100')\n",
    "    D = {}\n",
    "    for k in range(n):\n",
    "        print(k,n)\n",
    "        length = Y[k].shape[0]\n",
    "        for i in range(length):\n",
    "            for j in range(i+1,length):\n",
    "                Dis = j - i\n",
    "                Val = Y[k][i][j]\n",
    "                if Dis not in D:\n",
    "                    D[Dis] = Val, Val, Val, 1\n",
    "                else:\n",
    "                    MinV, MaxV, AvgV, Count = D[Dis]\n",
    "                    MinV = min(MinV,Val)\n",
    "                    MaxV = max(MaxV,Val)\n",
    "                    AvgV = AvgV + (Val - AvgV) / (Count + 1)\n",
    "                    D[Dis] = MinV, MaxV, AvgV, Count + 1\n",
    "    csvfile = open('./Data/data_distance.csv','w')\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for key in D:\n",
    "        csvwriter.writerow([key,'{0:.2f}'.format(D[key][0]),\\\n",
    "            '{0:.2f}'.format(D[key][1]),'{0:.2f}'.format(D[key][2])])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataV1():\n",
    "    # read data from Data/V0\n",
    "    # write data to Data/V1\n",
    "    path = 'Data/V1'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    for tag in ['100','80','20']:\n",
    "        n, X, Y = readTrainDataV0(tag)\n",
    "        csvfile = open('./Data/V1/train_data_'+tag+'.csv','w')\n",
    "        csvwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for k in range(n):\n",
    "            print(tag,k,n)\n",
    "            length = X[k][1]\n",
    "            str1 = X[k][2]\n",
    "            str2 = X[k][3]\n",
    "            y = Y[k]\n",
    "            \n",
    "            for i in range(length - 1):\n",
    "                # write string [i:]\n",
    "                # output string length is at least 2\n",
    "                _str1 = str1[i:]\n",
    "                _str2 = str2[i:]\n",
    "                csvwriter.writerow([length-i, _str1, _str2] + [list(y[i][i:])])\n",
    "                # reverse string\n",
    "                csvwriter.writerow([length-i, _str1[::-1], _str2[::-1]] + [list(y[length-1][i:])[::-1]])\n",
    "        csvfile.close()\n",
    "\n",
    "def readTrainDataV1(tag):\n",
    "    filename = 'Data/V0/train_data_'+tag+'.pickle'\n",
    "    for chunk in pd.read_csv(filename, chunksize=5000, quotechar='|', na_filter = False):\n",
    "        data = chunk.value\n",
    "        # do something with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-01 17:13:42  Reading training data\n",
      "2019-01-01 17:13:45  Formatting training data\n",
      "2019-01-01 17:13:45  Pickling formatted training data\n",
      "2019-01-01 17:13:49  Reading test data\n"
     ]
    }
   ],
   "source": [
    "#A/21 = \"ACDEFGHIKLMNPQRSTVWXY\"\n",
    "#B/8  = \"BEGHILST\"\n",
    "if __name__ == '__main__':\n",
    "    # Input files are train_input.csv, train_output.npz and test_input.csv in DATA_DIR\n",
    "    pickle_data_v0()\n",
    "#     pickle_data_close_v0(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TerminalIPythonApp] WARNING | Subcommand `ipython nbconvert` is deprecated and will be removed in future versions.\n",
      "[TerminalIPythonApp] WARNING | You likely want to use `jupyter nbconvert` in the future\n",
      "[NbConvertApp] Converting notebook data.ipynb to python\n",
      "[NbConvertApp] Writing 7458 bytes to data.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to=python data.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
